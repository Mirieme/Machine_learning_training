{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch  \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "#Get Device for training\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else  \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0047, 0.0064, 0.0000, 0.0000, 0.0403, 0.0129, 0.0113, 0.0712, 0.0377,\n",
      "         0.0000]], grad_fn=<ReluBackward0>)\n",
      "Predicted class: tensor([7])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device) # 1 image with 28x28 pixels\n",
    "logits = model(X) # Pass the image to the model\n",
    "print(logits)\n",
    "pred_probab = nn.Softmax(dim=1)(logits) # nn.Softmax(dim=1) is a function that converts logits to probabilities\n",
    "y_pred = pred_probab.argmax(1) # Get the predicted class of the image \n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function rand in torch:\n",
      "\n",
      "torch.rand = rand(...)\n",
      "    rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with random numbers from a uniform distribution\n",
      "    on the interval :math:`[0, 1)`\n",
      "    \n",
      "    The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "    \n",
      "    Keyword args:\n",
      "        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "        pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      "            the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.rand(4)\n",
      "        tensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n",
      "        >>> torch.rand(2, 3)\n",
      "        tensor([[ 0.8237,  0.5781,  0.6879],\n",
      "                [ 0.3816,  0.7249,  0.0998]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('torch.rand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3498, 0.7708],\n",
      "         [0.9219, 0.1663]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(1,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "image_input = torch.rand(3, 28, 28)\n",
    "print(image_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(image_input)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.1677, -0.2056, -0.1430, -0.1840, -0.0558,  0.2176,  0.3065, -0.1982,\n",
      "         -0.4467,  0.2053, -0.2286, -0.1961,  0.4031, -0.0521,  0.0547,  0.3832,\n",
      "         -0.0581,  0.0461,  0.0473,  0.3992],\n",
      "        [-0.5004,  0.0452, -0.5128, -0.0977,  0.0754,  0.1590, -0.2494, -0.3839,\n",
      "         -0.3106,  0.0957, -0.2398,  0.3879,  0.1831,  0.0143,  0.1793,  0.4734,\n",
      "         -0.1054,  0.4928,  0.1588,  0.4121],\n",
      "        [-0.5606,  0.0332, -0.6507, -0.3006,  0.2806,  0.1505,  0.0781, -0.1502,\n",
      "         -0.5643,  0.1053, -0.1817,  0.2531,  0.3976, -0.1308,  0.1596,  0.2778,\n",
      "         -0.1429,  0.0565, -0.2723,  0.1296]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2176, 0.3065, 0.0000, 0.0000,\n",
      "         0.2053, 0.0000, 0.0000, 0.4031, 0.0000, 0.0547, 0.3832, 0.0000, 0.0461,\n",
      "         0.0473, 0.3992],\n",
      "        [0.0000, 0.0452, 0.0000, 0.0000, 0.0754, 0.1590, 0.0000, 0.0000, 0.0000,\n",
      "         0.0957, 0.0000, 0.3879, 0.1831, 0.0143, 0.1793, 0.4734, 0.0000, 0.4928,\n",
      "         0.1588, 0.4121],\n",
      "        [0.0000, 0.0332, 0.0000, 0.0000, 0.2806, 0.1505, 0.0781, 0.0000, 0.0000,\n",
      "         0.1053, 0.0000, 0.2531, 0.3976, 0.0000, 0.1596, 0.2778, 0.0000, 0.0565,\n",
      "         0.0000, 0.1296]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential est un conteneur ordonné de modules. Les données sont transmises à tous les modules dans le même ordre que celui défini. Vous pouvez utiliser des conteneurs séquentiels pour créer un réseau rapide comme seq_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0224,  0.0349,  0.0165,  ...,  0.0138,  0.0355,  0.0107],\n",
      "        [-0.0114,  0.0177, -0.0093,  ..., -0.0041, -0.0289,  0.0148]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0296, -0.0179], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0405,  0.0096, -0.0232,  ..., -0.0367, -0.0076, -0.0423],\n",
      "        [ 0.0278,  0.0121,  0.0098,  ..., -0.0402, -0.0219, -0.0107]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0441,  0.0411], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0083, -0.0039,  0.0215,  ..., -0.0135, -0.0165, -0.0378],\n",
      "        [-0.0175,  0.0186,  0.0095,  ..., -0.0253, -0.0124,  0.0407]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0190, 0.0068], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 2])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[[1,2],[3,4],[5,6],[7,8]]])\n",
    "print(t.shape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[[1,2],[3,4],[5,6],[7,8]]])\n",
    "torch.flatten(t, start_dim = 0, end_dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 7, 7])\n",
      "torch.Size([32, 3136])\n",
      "torch.Size([32, 3136])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 64, 7, 7)\n",
    "# Aplatir à partir de la dimension 1 jusqu'à la dernière\n",
    "x_end_flat = torch.flatten(x, end_dim=1)\n",
    "x_start_flat = torch.flatten(x, start_dim=1)\n",
    "x_flat = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "print(x_end_flat.shape)\n",
    "print(x_start_flat.shape)\n",
    "print(x_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) # loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000002162659F220>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x0000021625F0F610>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2709, 0.0931, 0.2362],\n",
      "        [0.2709, 0.0931, 0.2362],\n",
      "        [0.2709, 0.0931, 0.2362],\n",
      "        [0.2709, 0.0931, 0.2362],\n",
      "        [0.2709, 0.0931, 0.2362]])\n",
      "tensor([0.2709, 0.0931, 0.2362])\n"
     ]
    }
   ],
   "source": [
    "# Optimizing the loss function : calculating the derivatives\n",
    "loss.backward() # calculate the gradients\n",
    "print(w.grad) # dloss/dw\n",
    "print(b.grad) # dloss/db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are reasons you might want to disable gradient tracking:\n",
    "\n",
    "To mark some parameters in your neural network as frozen parameters.\n",
    "\n",
    "To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
